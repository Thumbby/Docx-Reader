import streamlit as st
from langchain.document_loaders import Docx2txtLoader
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains import RetrievalQA

def main():
    # App title
    st.title("ğŸ“„ éœ€æ±‚æ–‡æ¡£åˆ†æ")

    # Sidebar for instructions and settings
    with st.sidebar:
        st.header("ä½¿ç”¨æ­¥éª¤")
        st.markdown("""
        1. ä¸Šä¼ éœ€æ±‚æ–‡æ¡£(ä»¥docxæ ¼å¼).
        2. è¯¢é—®ä¸æ–‡æ¡£ç›¸å…³çš„é—®é¢˜.
        3. ç³»ç»Ÿä¼šæ ¹æ®æ–‡æ¡£å°è¯•å›ç­”æ‚¨çš„é—®é¢˜.
        """)

        st.header("è®¾ç½®")
        st.markdown("""
        - **Embedding Model**: HuggingFace
        - **Retriever Type**: Similarity Search
        - **LLM**: DeepSeek R1 1.5b(Ollama)
        """)

    # Main file uploader section
    st.header("ğŸ“ ä¸Šä¼ æ‚¨çš„éœ€æ±‚æ–‡æ¡£")
    uploaded_file = st.file_uploader("æ­¤å¤„ä¸Šä¼ æ‚¨çš„éœ€æ±‚æ–‡æ¡£", type="docx")

    if uploaded_file is not None:
        st.success("éœ€æ±‚æ–‡æ¡£ä¸Šä¼ æˆåŠŸï¼æ­£åœ¨è§£æ...")

        with open("temp.docx", "wb") as f:
            f.write(uploaded_file.getvalue())

        loader = Docx2txtLoader('temp.docx')
        docs = loader.load()

        # Split the document into chunks
        text_splitter = SemanticChunker(HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2"))
        documents = text_splitter.split_documents(docs)
        
        # Instantiate the embedding model
        embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

        # Create vector store and retriever
        vector = FAISS.from_documents(documents, embedder)
        
        # Define the LLM and the prompt
        llm = Ollama(model="deepseek-r1:8b")
        prompt = """
        1. å‚è€ƒä¸‹åˆ—å†…å®¹å¹¶åœ¨æœ€åå›ç­”é—®é¢˜.
        2. å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆ,è¯·å›ç­”æ–‡æ¡£ä¸­æœªæåŠ,è¯·ä¸è¦è‡ªå·±ç¼–é€ ç­”æ¡ˆ.
        3. å°½å¯èƒ½ä»¥ä¸‹åˆ—å†…å®¹ä¸­åŸæ–‡æ¥å›ç­”é—®é¢˜
        å†…å®¹: {context}
        é—®é¢˜: {question}
        å›ç­”:"""
        QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)

        # Define the document and combination chains
        llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT, verbose=True)
        document_prompt = PromptTemplate(
            input_variables=["page_content", "source"],
            template="Context:\ncontent:{page_content}\nsource:{source}",
        )
        combine_documents_chain = StuffDocumentsChain(
            llm_chain=llm_chain,
            document_variable_name="context",
            document_prompt=document_prompt,
            verbose=True
        )
        
        # Question input and response display
        st.header("â“ å¼€å§‹è¯¢é—®ä¸æ–‡æ¡£ç›¸å…³çš„é—®é¢˜")
        
        # Initialize chat history
        if "messages" not in st.session_state:
            st.session_state.messages = []
            
         
        # Display chat messages from history on app rerun
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # React to user input
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜:"):
            # Display user message in chat message container
            with st.chat_message("user"):
                st.markdown(prompt)
                
            retriever = vector.as_retriever(search_type="similarity", search_kwargs={"k": 1})    
            retriever.invoke(prompt)
            qa = RetrievalQA(
                combine_documents_chain=combine_documents_chain,
                retriever=retriever,
                verbose=True,
                return_source_documents=True
            )
            
            # Add user message to chat history
            st.session_state.messages.append({"role": "user", "content": prompt})
            # Display assistant response in chat message container
            response = qa(prompt)["result"]
            with st.chat_message("assistant"):
                st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
                                
    else:
        st.info("è¯·å…ˆä¸Šä¼ æ–‡ä»¶")
    
if __name__ == "__main__":
    main()