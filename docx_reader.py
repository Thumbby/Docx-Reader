import streamlit as st
from langchain_community.document_loaders import Docx2txtLoader, UnstructuredMarkdownLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.chains.combine_documents import create_stuff_documents_chain, create_stuff_documents_chain
from langchain.chains import create_retrieval_chain, SequentialChain
from langchain.globals import set_verbose, set_debug
import os
import torch

torch.classes.__path__ = [os.path.join(torch.__path__[0], torch.classes.__file__)]
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE" 

# set_debug(True)

def chain_init():
    loader = Docx2txtLoader('temp.docx')
    docs = loader.load()

    # Split the document into chunks
    # text_splitter = SemanticChunker(OllamaEmbeddings(
    #     model="bge-m3",
    #     num_gpu=0,
    #     num_thread=4
    # ))
    # text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=100, separator="\n")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=256)
    documents = text_splitter.split_documents(docs)
    
    with open('chunk_output.txt', 'w', encoding='utf-8') as f:
    # print chunks
        f.write('\n'.join(map(str,documents)))
            
    # Instantiate the embedding model
    # embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    embedder = OllamaEmbeddings(
        model="bge-m3",
        # num_gpu=0,
        num_thread=4
    )

    # Create vector store and retriever
    vector = FAISS.from_documents(documents, embedder)
    retriever = vector.as_retriever(search_type="similarity", search_kwargs={"k": 3})
            
    # Define the LLM and the prompt
    llm = OllamaLLM(
        model="deepseek-r1:1.5b",
        # num_gpu=0,
        num_thread=4,
        temperature=0.1,
    )
    prompt = """
    1. å‚è€ƒä¸‹åˆ—å†…å®¹å¹¶åœ¨æœ€åå›ç­”é—®é¢˜.
    2. å¦‚æœä¸­ä¸åŒ…å«å›ç­”,,è¯·å›ç­”æ–‡æ¡£ä¸­æœªæåŠ,è¯·ä¸è¦è‡ªå·±ç¼–é€ ç­”æ¡ˆ.
    3. å°½å¯èƒ½ä»¥ä¸‹åˆ—å†…å®¹ä¸­åŸæ–‡æ¥å›ç­”é—®é¢˜
    4. å†…å®¹ä¸­å¯èƒ½å­˜åœ¨ä¸ç›¸å…³ä¿¡æ¯,è¯·ä½ ç­›é€‰å‡ºç›¸ä¼¼åº¦æœ€é«˜çš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜,å…¶ä¸­è‹¥å†…å®¹å’Œé—®é¢˜ä¸­å­˜åœ¨å¤§é‡ç›¸åŒå­—ç¬¦ä¸²,å¯è®¤ä¸ºæ˜¯ç›¸ä¼¼é«˜.
        ä¾‹å¦‚è¯¢é—®"ä¼šå‘˜çŠ¶æ€æ¥å£ä¿¡æ¯æ˜¯ä»€ä¹ˆ"è€Œå†…å®¹ä¸­åŒ…å«"ä¼šå‘˜çŠ¶æ€",åˆ™å°½å¯èƒ½ä»¥è¯¥å†…å®¹å›ç­”é—®é¢˜
    å†…å®¹: {context}
    é—®é¢˜: {input}
    å›ç­”:"""
    QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)

    # Define the document and combination chains
    llm_chain = QA_CHAIN_PROMPT | llm | StrOutputParser()
    document_prompt = PromptTemplate(
        input_variables=["page_content", "source"],
        template="Context:\ncontent:{page_content}\nsource:{source}",
    )
    combine_documents_chain = create_stuff_documents_chain(
        llm=llm,
        prompt= QA_CHAIN_PROMPT,
        document_variable_name="context",
        document_prompt=document_prompt,
    )
    
    retrieval_chain = create_retrieval_chain(
        retriever = retriever,
        combine_docs_chain = combine_documents_chain,    
    )
    
    return retrieval_chain
        
    

def main():
    # App title
    st.title("ğŸ“„ éœ€æ±‚æ–‡æ¡£åˆ†æ")

    # Sidebar for instructions and settings
    with st.sidebar:
        st.header("ä½¿ç”¨æ­¥éª¤")
        st.markdown("""
        1. ä¸Šä¼ éœ€æ±‚æ–‡æ¡£(ä»¥docxæ ¼å¼).
        2. è¯¢é—®ä¸æ–‡æ¡£ç›¸å…³çš„é—®é¢˜.
        3. ç³»ç»Ÿä¼šæ ¹æ®æ–‡æ¡£å°è¯•å›ç­”æ‚¨çš„é—®é¢˜.
        """)

        st.header("è®¾ç½®")
        st.markdown("""
        - **Embedding Model**: HuggingFace
        - **Retriever Type**: Similarity Search
        - **LLM**: DeepSeek R1 1.5b(Ollama)
        """)

        # Main file uploader section
        st.header("ğŸ“ ä¸Šä¼ æ‚¨çš„éœ€æ±‚æ–‡æ¡£")
        uploaded_file = st.file_uploader("æ­¤å¤„ä¸Šä¼ æ‚¨çš„éœ€æ±‚æ–‡æ¡£", type="docx")

    if uploaded_file is not None:
        st.success("éœ€æ±‚æ–‡æ¡£ä¸Šä¼ æˆåŠŸï¼æ­£åœ¨è§£æ...")

        with open("temp.docx", "wb") as f:
            f.write(uploaded_file.getvalue())
        
        retrieval_chain = chain_init()    

        # Question input and response display
        st.header("â“ å¼€å§‹è¯¢é—®ä¸æ–‡æ¡£ç›¸å…³çš„é—®é¢˜")
        
        # Initialize chat history
        if "messages" not in st.session_state:
            st.session_state.messages = []

         
        # Display chat messages from history on app rerun
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # React to user input
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜:"):
            # Display user message in chat message container
            with st.chat_message("user"):
                st.markdown(prompt)
            # Add user message to chat history
            st.session_state.messages.append({"role": "user", "content": prompt})    
                            
            response = retrieval_chain.invoke({"input": prompt})
            
            # Display assistant response in chat message container
            with st.chat_message("assistant"):
                st.markdown(response['answer'])
            st.session_state.messages.append({"role": "assistant", "content": response['answer']})
                                
    else:
        st.info("è¯·å…ˆä¸Šä¼ æ–‡ä»¶")
    
if __name__ == "__main__":
    main()